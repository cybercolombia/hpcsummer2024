{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e155b3",
   "metadata": {},
   "source": [
    "# Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf09e83-657f-4706-b66a-e24ee8c55ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "sns.set(font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf1bfd",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df781c45-7a7f-42e0-a53a-ae638559de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data = pd.read_csv(path+\"/code/data/train.txt\",header=None)\n",
    "data.columns = ['feature 1', 'feature 2', 'class']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e0163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax = plt.subplots(figsize=(6,6))\n",
    "sns.scatterplot(data=data, ax=ax, x='feature 1', y='feature 2', hue='class', style='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bff148",
   "metadata": {},
   "source": [
    "## Cost function and its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a4a69",
   "metadata": {},
   "source": [
    "The data points will be arranged as a matrix $X$\n",
    "$$X=\\left[\\begin{array}{ccc}1&x_1^{(1)}&x_2^{(1)}\\\\1&x_1^{(2)}&x_2^{(2)}\\\\ \\vdots&\\vdots&\\vdots\\\\ 1&x_1^{(m)}&x_2^{(m)} \\end{array}\\right],$$\n",
    "where $m$ is the number of data points, and $x_k^{(i)}$ represents feature $k$ of data point $i$, while the class at which each point belongs will be stored in vector $y$\n",
    "$$y=\\left[\\begin{array}{c}y^{(1)}\\\\y^{(2)}\\\\ \\vdots\\\\ y^{(m)} \\end{array}\\right].$$\n",
    "As the classifier is a One vs All scheme, the class vector has to be one-hot encoded: $0\\rightarrow (1,0,0)$, $1\\rightarrow (0,1,0)$, and $2\\rightarrow (0,0,1)$\n",
    "$$y=\\left[\\begin{array}{ccc}y_1^{(1)}&y_2^{(1)}&y_3^{(1)}\\\\y_1^{(2)}&y_2^{(2)}&y_3^{(2)}\\\\ \\vdots&\\vdots&\\vdots\\\\ y_1^{(m)}&y_2^{(m)}&y_3^{(m)} \\end{array}\\right].$$\n",
    "The weights are arranged in the matrix form\n",
    "$$\\Theta=\\left[\\begin{array}{ccc}\\theta_{01}&\\theta_{02}&\\theta_{03}\\\\\\theta_{11}&\\theta_{12}&\\theta_{13}\\\\ \\theta_{21}&\\theta_{22}&\\theta_{23}\\end{array}\\right],$$\n",
    "where $\\theta_{kl}$ is the weight associated with feature $k$ and model (class) $l$.\n",
    "\n",
    "<img src=\"figures/net.png\" width=150/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6606a-7165-4383-9714-ebc077b20f44",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`log_classifier.py`\n",
    "```python\n",
    "#Create matrix X and convert to array\n",
    "X = data.iloc[:,0:2].to_numpy()\n",
    "b = np.ones((X.shape[0],1))\n",
    "X = np.hstack((b,X))\n",
    "#Create the y one-hot encoding\n",
    "yl = data.iloc[:,2].to_numpy()\n",
    "y = np.array([[1 if yl[j]==i else 0 for i in [0,1,2]] for j in range(len(yl))])\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33145b",
   "metadata": {},
   "source": [
    "Therefore, once we have the weights $\\Theta$, we can obtain the outputs for the likelihood of point $x^{(i)}$ to belong to each of the 3 classes $a_1$, $a_2$, and $a_3$ \n",
    "$$X\\cdot\\Theta=\\left[\\begin{array}{ccc}1&x_1^{(1)}&x_2^{(1)}\\\\1&x_1^{(2)}&x_2^{(2)}\\\\ \\vdots&\\vdots&\\vdots\\\\ 1&x_1^{(m)}&x_2^{(m)} \\end{array}\\right]\\left[\\begin{array}{ccc}\\theta_{01}&\\theta_{02}&\\theta_{03}\\\\\\theta_{11}&\\theta_{12}&\\theta_{13}\\\\ \\theta_{21}&\\theta_{22}&\\theta_{23}\\end{array}\\right]=\\left[\\begin{array}{ccc}z_1^{(1)}&z_2^{(1)}&z_3^{(1)}\\\\z_1^{(2)}&z_2^{(2)}&z_3^{(2)}\\\\\\vdots&\\vdots&\\vdots\\\\z_1^{(m)}&z_2^{(m)}&z_3^{(m)}\\\\ \\end{array}\\right]=Z,$$\n",
    "where $z_l^{(i)}=\\sum_kx_k^{(i)}\\theta_{kl}$. Then\n",
    "</br>\n",
    "$$A=f_{\\Theta}(Z)=\\left[\\begin{array}{ccc}f_{\\Theta}(z_1^{(1)})&f_{\\Theta}(z_2^{(1)})&f_{\\Theta}(z_3^{(1)})\\\\f_{\\Theta}(z_1^{(2)})&f_{\\Theta}(z_2^{(2)})&f_{\\Theta}(z_3^{(2)})\\\\\\vdots&\\vdots&\\vdots\\\\f_{\\Theta}(z_1^{(m)})&f_{\\Theta}(z_2^{(m)})&f_{\\Theta}(z_3^{(m)})\\\\ \\end{array}\\right]=\\left[\\begin{array}{ccc}a_1^{(1)}&a_2^{(1)}&a_3^{(1)}\\\\a_1^{(2)}&a_2^{(2)}&a_3^{(2)}\\\\\\vdots&\\vdots&\\vdots\\\\a_1^{(m)}&a_2^{(m)}&a_3^{(m)}\\\\ \\end{array}\\right],$$\n",
    "where the activation function\n",
    "$$f_{\\Theta}(z_l^{(i)})=\\frac{1}{1+e^{-z_l^{(i)}}},$$\n",
    "is the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa7302",
   "metadata": {},
   "source": [
    "In order to train the logistic classifier we need to define a cost function, and the goal of the training proces is to find the weights $\\Theta$ that minimize such a cost function. The cost function used in the logistic regression is\n",
    "$$J(\\Theta)=-\\frac{1}{m}\\left[y:\\log(A)+(1-y):\\log(1-A)\\right]+\\frac{\\lambda}{2m}\\sum_{l=1}^3\\sum_{k=1}^2\\theta_{kl}^2,$$\n",
    "where $\\lambda$ is the regularization parameter, 1 is either a vector or a matrix of 1s, and \"A:B\" is the contraction between matrices $A$ and $B$, that is, perform the Hadamard product and then add all the elements.\n",
    "We have that\n",
    "$$y:\\log(A)=\\sum_{l=1}^n\\sum_{i=1}^my_l^{(i)}\\log(a_l^{(i)}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa02e4f",
   "metadata": {},
   "source": [
    "The gradient of the cost function is\n",
    "$$\\frac{\\partial J}{\\partial\\theta_{kl}}=-\\frac{1}{m}\\left[y:\\partial_{\\theta_{kl}}\\log(A)+(1-y):\\partial_{\\theta_{kl}}\\log(1-A)\\right]+\\frac{\\lambda}{m}\\theta_{kl}$$\n",
    "where\n",
    "$$y:\\partial_{\\theta_{kl}}\\log(A)=\\sum_i^my_l^{(i)}x_k^{(i)}(1-a_l^{(i)}),$$\n",
    "$$(1-y):\\partial_{\\theta_{kl}}\\log(1-A)=\\sum_i^m(1-y_l^{(i)})x_k^{(i)}a_l^{(i)}.$$\n",
    "Then\n",
    "$$\\frac{\\partial J}{\\partial\\theta_{kl}}=\\frac{1}{m}\\sum_i^mx_k^{(i)}\\left(a_l^{(i)}-y_l^{(i)}\\right)+\\frac{\\lambda}{m}\\theta_{kl}=\\frac{1}{m}x_k\\cdot(A_l-y_l)+\\frac{\\lambda}{m}\\theta_{kl}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b9042-db66-4115-908e-c3284868fe4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`lc_functions.py`\n",
    "```python\n",
    "#Logistic function\n",
    "def logistic(X, Theta):\n",
    "    dot = npmatmul.matrix_multiply(X, Theta)\n",
    "    return 1.0 / (1 + np.exp(-dot))\n",
    "\n",
    "#Cost function\n",
    "def J(X, y, Theta, lamb):\n",
    "    A = logistic(X,Theta)\n",
    "    ones = np.ones((X.shape[0],3))\n",
    "    A1 = ones - A\n",
    "    logA = np.log(A)\n",
    "    logA1 = np.log(A1)\n",
    "    y1 = ones - y\n",
    "    \n",
    "    dot1 = np.sum(np.multiply(y,logA))\n",
    "    dot2 = np.sum(np.multiply(y1,logA1))\n",
    "    fact = lamb / (2*y.shape[0])\n",
    "    thetac = np.copy(Theta)\n",
    "    thetac[:,0] = 0\n",
    "    reg = np.sum(fact*thetac**2)\n",
    "    \n",
    "    return -(dot1 + dot2)/y.shape[0] + reg\n",
    "\n",
    "#Gradient of the cost function\n",
    "def grad_J(X, y, Theta, lamb):\n",
    "    A = logistic(X,Theta)\n",
    "    Ay = A - y\n",
    "    fact = lamb / y.shape[0]\n",
    "    thetac = np.copy(Theta)\n",
    "    thetac[:,0] = 0\n",
    "    reg = fact*thetac\n",
    "    \n",
    "    return npmatmul.matrix_multiply(X.T,Ay)/y.shape[0] + reg\n",
    "```\n",
    "</div>          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d354f",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "In order to find the $\\Theta$ that minimizes $J(\\Theta)$ we use the gradient descent algorithm. The gradient descent algorithm is as follows:\n",
    "<ol>\n",
    "    <li>Initialize $\\Theta$.</li>\n",
    "    <li>Compute the gradient of the cost function $\\partial J/\\partial\\theta_{kl}$.</li>\n",
    "    <li>Update the weights as $\\theta_{kl}=\\theta_{kl}-\\alpha\\ \\partial J/\\partial\\theta_{kl}$, where $\\alpha$ is the convergence parameter of the algorithm.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcec66b-f043-4bf6-90d5-a7d80cefb869",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`lc_functions.py`\n",
    "```python\n",
    "#gradient descent\n",
    "def grad_desc(X, y, lamb, alpha, epochs, iprint):\n",
    "    epoch = []\n",
    "    history = []\n",
    "    #Initialize weight matrix\n",
    "    Theta = 0.1*np.ones((X.shape[1],3))\n",
    "    for e in range(epochs):\n",
    "        Theta = Theta - alpha*grad_J(X,y,Theta,lamb)\n",
    "        if e%iprint == 0:\n",
    "            epoch.append(e)\n",
    "            history.append(J(X,y,Theta,lamb))\n",
    "    return Theta, epoch, history\n",
    "```\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874901dc-031b-4067-9e99-f842c1b26e78",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`log_classifier.py`\n",
    "```python\n",
    "lamb = 0.1; # Regularization parameter\n",
    "alpha = 0.005; # Convergence parameter\n",
    "num_iters = 100000;\n",
    "\n",
    "#Run gradient descent\n",
    "Th, ep, hist = grad_desc(X,y,lamb,alpha,num_iters,100)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303ef18-1df2-4bce-8d9e-246116ec8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 1 -c 8 python3.10 code/log_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f8a2f-f2f1-4d17-a4bb-c27c58e8cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "Th = []\n",
    "with open(path+\"/code/data/model.dat\",\"r\") as mdata:\n",
    "    for line in mdata:\n",
    "        v = list(map(float,line.split()))\n",
    "        Th.append(v)\n",
    "Th = np.asarray(Th)\n",
    "\n",
    "#Load convergence data\n",
    "ep = []\n",
    "hist = []\n",
    "with open(path+\"/code/data/convergence.dat\",\"r\") as cdata:\n",
    "    for line in cdata:\n",
    "        v = list(map(float,line.split()))\n",
    "        ep.append(v[0])\n",
    "        hist.append(v[1])\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b2aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot cost gradient descent convergence\n",
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.plot(ep,hist,linestyle='',marker='o')\n",
    "ax.set_xlabel('epoch',fontsize=20)\n",
    "ax.set_ylabel('J',fontsize=20)\n",
    "#ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5be9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The boundary is found when logistic(theta_min, x) = 0.5\n",
    "# which is the same as x . theta_min = 0\n",
    "def decisionBoundary(theta, xin, xfin, N):\n",
    "    Dx = (xfin-xin) / N\n",
    "    a = theta[1] / theta[2]\n",
    "    b = theta[0] / theta[2]\n",
    "    boundary = []\n",
    "    for x in np.arange(xin, xfin+1, Dx):\n",
    "        boundary.append([x, -a*x-b])\n",
    "    return pd.DataFrame(boundary, columns=['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbf1 = decisionBoundary(Th.T[0],0,3.5,100)\n",
    "dbf2 = decisionBoundary(Th.T[1],0,3.5,100)\n",
    "dbf3 = decisionBoundary(Th.T[2],0,3.5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba04367",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.scatterplot(data=data, ax=ax, x='feature 1', y='feature 2', hue='class', style='class')\n",
    "sns.lineplot(data=dbf1, ax=ax, x='x1', y='x2', color='g')\n",
    "sns.lineplot(data=dbf2, ax=ax, x='x1', y='x2', color='r')\n",
    "sns.lineplot(data=dbf3, ax=ax, x='x1', y='x2', color='b')\n",
    "plt.xlim(0,3.5)\n",
    "plt.ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a6a30",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b48330",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(path+\"/code/data/test.txt\",header=None)\n",
    "test_data.columns = ['feature 1', 'feature 2', 'class']\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(figsize=(6,6))\n",
    "sns.scatterplot(data=test_data, ax=ax, x='feature 1', y='feature 2', hue='class', style='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13005d8-33ad-4e89-8ab4-59a65098dea6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`test_accuracy.py`\n",
    "```python\n",
    "#Create matrix Xtest and convert to array\n",
    "Xt = test_data.iloc[:,0:2].to_numpy()\n",
    "bt = np.ones((Xt.shape[0],1))\n",
    "Xt = np.hstack((bt,Xt))\n",
    "#Create the ytest vector\n",
    "yt = test_data.iloc[:,2].to_numpy()[np.newaxis].T #create a new axis to convert a 1d array\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d3f74-a38a-4f2c-90e7-0b96f597c214",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`lc_functions.py`\n",
    "```python\n",
    "#Function to predict all the samples in the training set with the trained model\n",
    "def logi(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def predictOneVsAll(X, Theta):\n",
    "    M = logi(npmatmul.matrix_multiply_omp(X,Theta))\n",
    "    p = np.array([[np.where(row == np.max(row))[0][0]] for row in M])\n",
    "    return p\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1283343-6263-4beb-a6d5-852d7ebfe399",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "`test_accuracy.py`\n",
    "```python\n",
    "pred = predictOneVsAll(Xt,Th)\n",
    "print(\"Training set accuracy: {0:.3f}\".format(np.mean(pred==yt)))\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 1 -c 8 python3.10 code/test_accuracy.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
