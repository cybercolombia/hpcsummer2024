{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ef1e1b-eb29-41f7-b7ff-04854419c6a8",
   "metadata": {},
   "source": [
    "# ============ Logistic Regression ============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0199269-8153-426f-ab3c-135cb6e43db5",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "Imagine an apple plantation. The farmers have identified that it is possible to tell bad from good apple in an approximate way by their weight ($W$): on average, bad apples weight less than good apples. However, they are not sure about the limit, that is from what value of $W$ an apple should be considered to be good, as in some rare cases they find good a apple that weight less than a bad apple. The task at hand is to create an automated system that, given the weight of an apple decides whether or not it is a good one, having the lowest error rate possible. \n",
    "\n",
    "The previous is an example of binary classification, in which elements (apples) can be classified in either of two classes (good or bad). One way to acheive our goal is to start by preparing the data in the following way: \n",
    "\n",
    "1) Take a large sample of apples, measure the weight $W$ of each apple, see if it is a good or bad apple and put a tag on it. We will say that $W$ is the *feature* that we meassure, and the state (good/bad) of the apple is the *category*.\n",
    "One ends up with a table like the following \n",
    "\n",
    "|Feature(W in g)|category|\n",
    "|:-:|---|\n",
    "|120|bad|\n",
    "|200|good|\n",
    "|134|bad|\n",
    "|165|good|\n",
    "|170|bad|\n",
    "|154|bad|\n",
    "|213|good|\n",
    "\n",
    "2) Give a numeric value to the categories, for example bad:0, good:1. This will separate the values in the category dimension\n",
    "   <video width=\"480\" height=\"360\" src=\"figures/animations/separate.mp4\" controls>\n",
    "   </video>\n",
    "\n",
    "After preparing the data we proceed to create a model, that is, a function that receives $W$ as input and gives a category (0,1) as output. A convenient function to choose in this case, as we will se later, is a **logistic function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42d7a8-4e94-495a-959c-a69d35a3136f",
   "metadata": {},
   "source": [
    "## The logistic function\n",
    "The logistic function of a variable $x$ is defined as\n",
    "$$ f(x)=\\frac{1}{1+e^{-n(x-m)}},$$\n",
    "where $n$ and $m$ are constants and $e$ is the Euler's number.\n",
    "\n",
    "<img src=\"figures/logistic_f.png\" alt=\"Moore\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e386c-333b-4cd7-a66e-e48bdd1721fc",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "A logistic regression is the process of finding the parameters $m$ and $n$ of the logistic function such that the curve passes close to a set of data points. In this way, the resulting logistic function will be our model, which receives the value of the feature for a data point as an input, and returns a category for that data point as an output.\n",
    "\n",
    "<video width=\"480\" height=\"360\" src=\"figures/animations/fit1d.mp4\" controls>\n",
    "</video>\n",
    "\n",
    "The value of the function is interpreted as the probability for the data point to belong to category 1. We will refer to the logistic funtion as the *activation* function, while refering to the regression process as the *training* of the model.\n",
    "\n",
    "Before proceeding we make a slight change of notation to use a more standard one\n",
    "$$f(x)=\\frac{1}{1+e^{-n(x-m)}}\\rightarrow\\frac{1}{1+e^{-(\\theta_0+\\theta_1 x)}}.$$\n",
    "where $\\theta_i$ are the parameters obtained with the regression.\n",
    "\n",
    "Likewise, the data has to be prepared in matrix form\n",
    "|data|$X$|$y$|\n",
    "|:-:|:-:|:-:|\n",
    "|$$ \\left[\\begin{array}{2}x^{(1)}& y^{(1)}\\\\x^{(2)}& y^{(2)}\\\\ \\vdots&\\vdots\\\\x^{(m)}& y^{(m)}\\\\ \\end{array}\\right]\\rightarrow$$|$$\\left[\\begin{array}{2}1&x^{(1)}\\\\1&x^{(2)}\\\\ \\vdots&\\vdots\\\\1&x^{(m)}\\\\ \\end{array}\\right]$$|$$\\left[\\begin{array}{2}y^{(1)}\\\\y^{(2)}\\\\ \\vdots\\\\y^{(m)}\\\\ \\end{array}\\right]$$|\n",
    "\n",
    "where we assume there are $m$ data points to train our model, $x^{(i)}$ is the feature value of data point $i$, and $y^{(i)}$ its category (0 or 1). Meanwhile, the parameters are also treated as a vector\n",
    "$$ \\Theta=\\left[\\begin{array}{1}\\theta_0\\\\ \\theta_1\\end{array}\\right].$$\n",
    "\n",
    "In this way, we can write \n",
    "$$\\theta_0+\\theta_1 x^{(i)}=\\left[\\begin{array}{2}1&x^{(i)}\\end{array}\\right]\\left[\\begin{array}{1}\\theta_0\\\\ \\theta_1\\end{array}\\right]=z^{(i)}$$ \n",
    "and the activation function as\n",
    "$$f_\\Theta(X)=\\frac{1}{1+e^{-Z}},\\ \\ \\ Z=X\\cdot\\Theta=\\left[\\begin{array}{1}z^{(1)}\\\\ \\vdots\\\\z^{(m)}\\end{array}\\right].$$\n",
    "The activation function is then writen as a vector\n",
    "$$A=f_\\Theta(X)=\\left[\\begin{array}{1}\\frac{1}{1+e^{-z^{(1)}}}\\\\ \\vdots\\\\\\frac{1}{1+e^{-z^{(m)}}}\\end{array}\\right]=\\left[\\begin{array}{1}a^{(1)}\\\\ \\vdots\\\\a^{(m)}\\end{array}\\right].$$\n",
    "Note that the computation of the activation function involves a matrix multiplication as well as element-wise operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e28421-a392-45bb-82ee-f33416e5ac23",
   "metadata": {},
   "source": [
    "## Cost function and gradient descent\n",
    "A regression involves finding the values of $\\Theta$ that minimize the average distance between the activation function and the data points, therefore it is desirable for this function to be convex. In case of the logistic regression, the cost function to minimize is \n",
    "$$J(\\Theta)=-\\frac{1}{m}\\left[y\\cdot\\log(A)+(1-y)\\cdot\\log(1-A)\\right]+\\frac{\\lambda}{2m}\\theta_{1}^2,$$\n",
    "where $\\lambda$ is called the *regularization* parameter, and is there to avoid the fit to become too close to the points (overfitting).\n",
    "\n",
    "To find the lowest value of $J$, we start by finding its gradient with respect to the parameters $\\Theta$\n",
    "$$\\frac{\\partial J}{\\partial\\theta_{0}}=\\frac{1}{m}\\sum_i^m\\left(a^{(i)}-y^{(i)}\\right),$$\n",
    "$$\\frac{\\partial J}{\\partial\\theta_{1}}=\\frac{1}{m}\\sum_i^mx^{(i)}\\left(a^{(i)}-y^{(i)}\\right)+\\frac{\\lambda}{m}\\theta_{1},$$\n",
    "and then proceede to move iteratively the parameters in the direction of the gradient. The gradient descent algorithm is as follows:\n",
    "<ol>\n",
    "    <li>Initialize $\\Theta$.</li>\n",
    "    <li>Compute the gradient of the cost function $\\partial J/\\partial\\theta_{j}$.</li>\n",
    "    <li>Update the weights as $\\theta_{j}=\\theta_{j}-\\alpha\\ \\partial J/\\partial\\theta_{j}$, where $\\alpha$ is the convergence parameter of the algorithm.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de651238-452d-4108-998b-03b226f202e5",
   "metadata": {},
   "source": [
    "## Logistic regression with two features\n",
    "When we have two features, we go from a single independent axis to a two dimensional space $x\\rightarrow(x_1,x_2)$. The main difference is that our data point matrix gets a new dimension  \n",
    "|data|$X$|$y$|\n",
    "|:-:|:-:|:-:|\n",
    "|$$ \\left[\\begin{array}{3}x_1^{(1)}&x_2^{(1)}&y^{(1)}\\\\x_1^{(2)}&x_2^{(2)}& y^{(2)}\\\\ \\vdots&\\vdots&\\vdots\\\\x_1^{(m)}&x_2^{(m)}& y^{(m)}\\\\ \\end{array}\\right]\\rightarrow$$|$$\\left[\\begin{array}{3}1&x_1^{(1)}&x_2^{(1)}\\\\1&x_1^{(2)}&x_2^{(2)}\\\\ \\vdots&\\vdots&\\vdots\\\\1&x_1^{(m)}&x_2^{(m)}\\\\ \\end{array}\\right]$$|$$\\left[\\begin{array}{2}y^{(1)}\\\\y^{(2)}\\\\ \\vdots\\\\y^{(m)}\\\\ \\end{array}\\right]$$|\n",
    "\n",
    "and we get a new parameter\n",
    "$$ \\Theta=\\left[\\begin{array}{1}\\theta_0\\\\ \\theta_1\\\\ \\theta_2\\end{array}\\right].$$\n",
    "\n",
    "<video width=\"640\" height=\"480\" src=\"figures/animations/fit2d.mp4\" controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d1438-7151-496e-83e2-68a5b290332e",
   "metadata": {},
   "source": [
    "## Multiple logistic regression\n",
    "When we have more thatn two classes, we can train several binary logistic classifiers. Each classifier will separate between a particular category $k$ and the rest\n",
    "\n",
    "<video width=\"480\" height=\"360\" src=\"figures/animations/multiway.mp4\" controls>\n",
    "</video>\n",
    "\n",
    "As we effectively have several models now, such that model $k$ distinguishes between category $k$ and the rest, we need several $y$ vectors, one to train each model. Assume we have 3 categories: 1, 2, and 3 as the example above, and our $y$ vector looks like this\n",
    "$$y=\\left[\\begin{array}{cc}y^{(1)}\\\\y^{(2)}\\\\y^{(3)}\\\\y^{(4)}\\\\ \\end{array}\\right]=\\left[\\begin{array}{cc}1\\\\3\\\\1\\\\2\\\\ \\end{array}\\right].$$\n",
    "\n",
    "We will use a technique called *one-hot encoding* to create 3 vectors, each one will treat its associated category as 1 and the other two as 0. We do the following transformation on the categories\n",
    "$$1\\rightarrow(1,0,0)$$\n",
    "$$2\\rightarrow(0,1,0)$$\n",
    "$$3\\rightarrow(0,0,1)$$\n",
    "to obtain\n",
    "$$y=\\left[\\begin{array}{3}y_1^{(1)}&y_2^{(1)}&y_3^{(1)}\\\\y_1^{(2)}&y_2^{(2)}&y_3^{(2)}\\\\y_1^{(3)}&y_2^{(3)}&y_3^{(3)}\\\\y_1^{(4)}&y_2^{(4)}&y_3^{(4)}\\\\ \\end{array}\\right]=\\left[\\begin{array}{ccc}1&0&0\\\\ 0&0&1\\\\ 1&0&0\\\\ 0&1&0\\\\ \\end{array}\\right].$$\n",
    "\n",
    "In the same way the number of parameters increases, as in this case each of the 3 models needs its own set of parameters\n",
    "$$\\Theta=\\left[\\begin{array}{ccc}\\theta_{01}&\\theta_{02}&\\theta_{03}\\\\\\theta_{11}&\\theta_{12}&\\theta_{13}\\\\ \\theta_{21}&\\theta_{22}&\\theta_{23}\\end{array}\\right],$$\n",
    "where $\\theta_{kl}$ is the parameter of the $k$th feature for the $l$th model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd3320-612a-4888-b462-12c33c62b38b",
   "metadata": {},
   "source": [
    "## Computational requirements\n",
    "We have seen that the training of a model requires several matrix multiplications plus element-wise opperations each time we perform the gradient descent, which might be performed hundreds of thousands of times. Add to that the size of the matrices, which depend on the number of data points used in the training process and the number of features (dimensions) of the problem.\n",
    "\n",
    "Imagine a classifier that has to distinguish between 5 categories, based on a 100 dimensional feature space. 1000000 data points have been gathered to train the model. In such a case, which is by no means a relatively big case, we would require $5\\times100\\times 10^6=5\\times10^8$ operations just to compute $Z$. Then, around $3\\times5\\times10^6=1.5\\times10^7$ to compute $A$ plus around $10^8$ more operations to get $J$ and its gradient. All this operations might have to be performed around $10^5$ times for a simple case, which amounts to an order of $10^{13}$ operations to train the model.\n",
    "\n",
    "A modern single core in a processor might be able to perform around $10^{9}$ operations per second, which means that it will take approximately $10^4$ seconds, that is 3 to 4 hours for it to train the simple model previously described. Models that tackle real problems tend to be much larger than this, and it is very important to reduce the training and operation times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
